{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title üõ†Ô∏è Step 1: Find Your Valid Model & Reset Libraries\n",
        "# Uninstall broken versions and install fresh\n",
        "!pip uninstall -y google-generativeai langchain-google-genai langgraph langchain\n",
        "!pip install -U google-generativeai langchain-google-genai langgraph langchain_community duckduckgo-search requests\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# 1. Get API Key\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google Gemini API Key: \")\n",
        "\n",
        "api_key = os.environ[\"GOOGLE_API_KEY\"]\n",
        "\n",
        "# 2. RAW API Query (Bypassing Python SDK to get the TRUTH)\n",
        "print(\"\\nüì° Connecting to Google API servers directly...\")\n",
        "url = f\"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}\"\n",
        "response = requests.get(url)\n",
        "\n",
        "valid_model = None\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    print(\"\\n‚úÖ AVAILABLE MODELS (Supported by your Key):\")\n",
        "\n",
        "    # Filter for models that support 'generateContent'\n",
        "    chat_models = [\n",
        "        m for m in data.get('models', [])\n",
        "        if 'generateContent' in m.get('supportedGenerationMethods', [])\n",
        "    ]\n",
        "\n",
        "    # Prioritize Stable Flash -> Stable Pro -> Legacy\n",
        "    # We look for '002' or '001' specifically to avoid 404s on aliases\n",
        "    priority_order = [\n",
        "        \"gemini-1.5-flash-002\",\n",
        "        \"gemini-1.5-flash-001\",\n",
        "        \"gemini-1.5-pro-002\",\n",
        "        \"gemini-1.5-pro-001\",\n",
        "        \"gemini-1.0-pro\"\n",
        "    ]\n",
        "\n",
        "    # Create a map for easy lookup\n",
        "    available_names = [m['name'].replace(\"models/\", \"\") for m in chat_models]\n",
        "\n",
        "    for p in priority_order:\n",
        "        if p in available_names:\n",
        "            valid_model = p\n",
        "            break\n",
        "\n",
        "    # Fallback: Just take the first Flash model found\n",
        "    if not valid_model:\n",
        "        for name in available_names:\n",
        "            if \"flash\" in name and \"exp\" not in name: # Avoid experimental (quota issues)\n",
        "                valid_model = name\n",
        "                break\n",
        "\n",
        "    # Final Fallback: Take ANYTHING\n",
        "    if not valid_model and available_names:\n",
        "        valid_model = available_names[0]\n",
        "\n",
        "    print(f\"üéØ AUTO-SELECTED BEST MODEL: {valid_model}\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå API Error: {response.status_code} - {response.text}\")\n",
        "\n",
        "# Store for next cell\n",
        "if valid_model:\n",
        "    os.environ[\"VALID_MODEL_ID\"] = valid_model\n",
        "    print(\"\\n‚úÖ Setup Complete. Run the next cell.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è CRITICAL: No models found. Check your API Key permissions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "C8I4lyrfAKDV",
        "outputId": "2df26c97-d8f0-4cb5-8443-c4b5dcb890f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: google-generativeai 0.8.5\n",
            "Uninstalling google-generativeai-0.8.5:\n",
            "  Successfully uninstalled google-generativeai-0.8.5\n",
            "Found existing installation: langchain-google-genai 2.0.10\n",
            "Uninstalling langchain-google-genai-2.0.10:\n",
            "  Successfully uninstalled langchain-google-genai-2.0.10\n",
            "Found existing installation: langgraph 1.0.1\n",
            "Uninstalling langgraph-1.0.1:\n",
            "  Successfully uninstalled langgraph-1.0.1\n",
            "Found existing installation: langchain 0.3.27\n",
            "Uninstalling langchain-0.3.27:\n",
            "  Successfully uninstalled langchain-0.3.27\n",
            "Collecting google-generativeai\n",
            "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langgraph\n",
            "  Using cached langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.12/dist-packages (8.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.25.8)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-3.0.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_google_genai-3.0.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_google_genai-3.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain-google-genai)\n",
            "  Using cached langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-3.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Using cached langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.3.80)\n",
            "  Using cached langchain_google_genai-2.1.11-py3-none-any.whl.metadata (6.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Using cached langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Using cached langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Using cached langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Using cached langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Using cached langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Using cached langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Using cached langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
            "  Using cached langgraph_prebuilt-1.0.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain<2.0.0,>=0.3.27 (from langchain_community)\n",
            "  Using cached langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.42)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.0)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (0.15.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain-1.0.7-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached langchain-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached langchain-1.0.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain-1.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain-1.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.0)\n",
            "INFO: pip is looking at multiple versions of langgraph-prebuilt to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
            "  Using cached langgraph_prebuilt-1.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Using cached langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
            "  Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "INFO: pip is still looking at multiple versions of langgraph-prebuilt to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Using cached langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Using cached langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "  Using cached langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\n",
            "  Using cached langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.26 (from langchain_community)\n",
            "  Using cached langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
            "  Using cached langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langsmith<0.4,>=0.1.125 (from langchain_community)\n",
            "  Using cached langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.25 (from langchain_community)\n",
            "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.24 (from langchain_community)\n",
            "  Using cached langchain-0.3.24-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.23 (from langchain_community)\n",
            "  Using cached langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.21 (from langchain_community)\n",
            "  Using cached langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\n",
            "  Using cached langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.20 (from langchain_community)\n",
            "  Using cached langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.19 (from langchain_community)\n",
            "  Using cached langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.18 (from langchain_community)\n",
            "  Using cached langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.16 (from langchain_community)\n",
            "  Using cached langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Using cached langchain-0.3.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.15 (from langchain_community)\n",
            "  Using cached langchain-0.3.15-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langsmith<0.3,>=0.1.125 (from langchain_community)\n",
            "  Using cached langsmith-0.2.11-py3-none-any.whl.metadata (14 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-core<0.4.0,>=0.3.37 (from langchain-google-genai)\n",
            "  Using cached langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Using cached langchain_core-0.3.78-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Using cached langchain_core-0.3.77-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Using cached langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Using cached langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "  Using cached langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.73-py3-none-any.whl.metadata (5.8 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_core-0.3.71-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.70-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.69-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.67-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.64-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai)\n",
            "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting langchain<0.4.0,>=0.3.14 (from langchain_community)\n",
            "  Using cached langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.13 (from langchain_community)\n",
            "  Using cached langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.12 (from langchain_community)\n",
            "  Using cached langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.11 (from langchain_community)\n",
            "  Using cached langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain_community)\n",
            "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.10 (from langchain_community)\n",
            "  Using cached langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.8 (from langchain_community)\n",
            "  Using cached langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Using cached langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting numpy<3,>=1.26.2 (from langchain_community)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting SQLAlchemy<2.0.36,>=1.4 (from langchain_community)\n",
            "  Using cached SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.7 (from langchain_community)\n",
            "  Using cached langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "  Using cached langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.6 (from langchain_community)\n",
            "  Using cached langchain-0.3.6-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.4-py3-none-any.whl.metadata (2.9 kB)\n",
            "  Using cached langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.4 (from langchain_community)\n",
            "  Using cached langchain-0.3.5-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Using cached langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.3 (from langchain_community)\n",
            "  Using cached langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.1 (from langchain_community)\n",
            "  Using cached langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Using cached langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.0 (from langchain_community)\n",
            "  Using cached langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.17 (from langchain_community)\n",
            "  Using cached langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.2.18-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_community-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_community-0.2.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_community-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n",
            "  Using cached langchain_community-0.2.2-py3-none-any.whl.metadata (8.9 kB)\n",
            "  Using cached langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "  Using cached langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Using cached langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Using cached langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Using cached langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Using cached langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Using cached langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Using cached langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Using cached langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
            "  Using cached langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n",
            "  Using cached langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.19-py3-none-any.whl.metadata (7.9 kB)\n",
            "  Using cached langchain_community-0.0.18-py3-none-any.whl.metadata (7.9 kB)\n",
            "  Using cached langchain_community-0.0.17-py3-none-any.whl.metadata (7.9 kB)\n",
            "  Using cached langchain_community-0.0.16-py3-none-any.whl.metadata (7.8 kB)\n",
            "  Using cached langchain_community-0.0.15-py3-none-any.whl.metadata (7.6 kB)\n",
            "  Using cached langchain_community-0.0.14-py3-none-any.whl.metadata (7.5 kB)\n",
            "  Using cached langchain_community-0.0.13-py3-none-any.whl.metadata (7.5 kB)\n",
            "  Using cached langchain_community-0.0.12-py3-none-any.whl.metadata (7.5 kB)\n",
            "  Using cached langchain_community-0.0.11-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Using cached langchain_community-0.0.10-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Using cached langchain_community-0.0.8-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Using cached langchain_community-0.0.7-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Using cached langchain_community-0.0.6-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Using cached langchain_community-0.0.5-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Using cached langchain_community-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Using cached langchain_community-0.0.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Using cached langchain_community-0.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_community-0.0.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting langgraph\n",
            "  Using cached langgraph-1.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "  Using cached langgraph-1.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.1)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.75->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.3.1)\n",
            "Using cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
            "Using cached langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "Using cached langgraph-1.0.1-py3-none-any.whl (155 kB)\n",
            "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "Installing collected packages: langchain, google-generativeai, langgraph, langchain-google-genai\n",
            "Successfully installed google-generativeai-0.8.5 langchain-0.3.27 langchain-google-genai-2.0.10 langgraph-1.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "0eaad697b848441d8a460f2922689968"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Google Gemini API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "\n",
            "üì° Connecting to Google API servers directly...\n",
            "\n",
            "‚úÖ AVAILABLE MODELS (Supported by your Key):\n",
            "üéØ AUTO-SELECTED BEST MODEL: gemini-2.5-flash\n",
            "\n",
            "‚úÖ Setup Complete. Run the next cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üöÄ Step 2: Run \"DealCloser\" Agent\n",
        "# This uses the VALID_MODEL_ID found above to prevent 404s.\n",
        "\n",
        "import os\n",
        "import time\n",
        "from typing import TypedDict, List\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "# Retrieve the validated model ID\n",
        "active_model = os.environ.get(\"VALID_MODEL_ID\", \"gemini-1.5-flash\")\n",
        "print(f\"ü§ñ Initializing Agent with Model: {active_model}\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=active_model,\n",
        "    temperature=0.3,\n",
        "    max_retries=2\n",
        ")\n",
        "\n",
        "search_tool = DuckDuckGoSearchRun()\n",
        "\n",
        "# --- 2. DEFINE STATE ---\n",
        "class AgentState(TypedDict):\n",
        "    company_name: str\n",
        "    target_role: str\n",
        "    research_data: str\n",
        "    strategy_points: str\n",
        "    final_email: str\n",
        "    messages: List[str]\n",
        "\n",
        "# --- 3. AGENTS (With Rate Limit Protection) ---\n",
        "\n",
        "def research_agent(state: AgentState):\n",
        "    print(f\"üîé Researching: {state['company_name']}...\")\n",
        "    query = f\"latest strategic business news {state['company_name']} 2024 2025\"\n",
        "    try:\n",
        "        # Adding a timeout wrapper could be useful here, but basic invoke is usually fine\n",
        "        search_result = search_tool.invoke(query)\n",
        "    except Exception as e:\n",
        "        search_result = f\"Search unavailable: {e}\"\n",
        "\n",
        "    time.sleep(1) # Safety pause for Free Tier\n",
        "    return {\n",
        "        \"research_data\": search_result,\n",
        "        \"messages\": [f\"Research complete\"]\n",
        "    }\n",
        "\n",
        "def analysis_agent(state: AgentState):\n",
        "    print(\"üß† Analyzing data...\")\n",
        "    prompt = f\"\"\"\n",
        "    Analyze this news for {state['company_name']}:\n",
        "    {state['research_data']}\n",
        "\n",
        "    Identify 2 key business problems they are trying to solve.\n",
        "    Return ONLY bullet points.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        content = response.content\n",
        "    except Exception as e:\n",
        "        content = f\"Analysis skipped: {e}\"\n",
        "\n",
        "    time.sleep(1)\n",
        "    return {\n",
        "        \"strategy_points\": content,\n",
        "        \"messages\": [f\"Analysis complete\"]\n",
        "    }\n",
        "\n",
        "def copywriter_agent(state: AgentState):\n",
        "    print(\"‚úçÔ∏è Drafting email...\")\n",
        "    prompt = f\"\"\"\n",
        "    Write a cold email to a {state['target_role']} at {state['company_name']}.\n",
        "    Mention these priorities to build relevance:\n",
        "    {state['strategy_points']}\n",
        "\n",
        "    Keep it under 100 words.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        content = response.content\n",
        "    except Exception as e:\n",
        "        content = f\"Drafting skipped: {e}\"\n",
        "\n",
        "    return {\n",
        "        \"final_email\": content,\n",
        "        \"messages\": [f\"Drafting complete\"]\n",
        "    }\n",
        "\n",
        "# --- 4. COMPILE ---\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"researcher\", research_agent)\n",
        "workflow.add_node(\"analyst\", analysis_agent)\n",
        "workflow.add_node(\"copywriter\", copywriter_agent)\n",
        "\n",
        "workflow.set_entry_point(\"researcher\")\n",
        "workflow.add_edge(\"researcher\", \"analyst\")\n",
        "workflow.add_edge(\"analyst\", \"copywriter\")\n",
        "workflow.add_edge(\"copywriter\", END)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "\n",
        "print(\"‚úÖ Agent System Ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oum0CiHXFkdj",
        "outputId": "aed24bfa-ebb7-4000-febe-3f39ecf6de96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Initializing Agent with Model: gemini-2.5-flash\n",
            "‚úÖ Agent System Ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚ö° Run the Agent\n",
        "target_company = \"Spotify\" # @param {type:\"string\"}\n",
        "target_person_role = \"VP of Marketing\" # @param {type:\"string\"}\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"final_run_v2\"}}\n",
        "\n",
        "inputs = {\n",
        "    \"company_name\": target_company,\n",
        "    \"target_role\": target_person_role,\n",
        "    \"research_data\": \"\",\n",
        "    \"strategy_points\": \"\",\n",
        "    \"final_email\": \"\",\n",
        "    \"messages\": []\n",
        "}\n",
        "\n",
        "print(f\"üöÄ Starting sequence for {target_company}...\")\n",
        "result = app.invoke(inputs, config=config)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéØ FINAL EMAIL DRAFT\")\n",
        "print(\"=\"*50)\n",
        "print(result['final_email'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "yCCnARlFBoSH",
        "outputId": "821243da-5516-4308-8984-27b5671406d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Verifying model access...\n",
            "‚úÖ Selected Stable Model: gemini-2.0-flash-exp\n",
            "‚úÖ Agent System Compiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚ö° Run Your Agent (Test)\n",
        "target_company = \"Spotify\" # @param {type:\"string\"}\n",
        "target_person_role = \"VP of Marketing\" # @param {type:\"string\"}\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"session_final_v1\"}}\n",
        "\n",
        "inputs = {\n",
        "    \"company_name\": target_company,\n",
        "    \"target_role\": target_person_role,\n",
        "    \"research_data\": \"\",\n",
        "    \"strategy_points\": \"\",\n",
        "    \"final_email\": \"\",\n",
        "    \"messages\": []\n",
        "}\n",
        "\n",
        "print(f\"üöÄ Starting sequence for {target_company}...\\n\")\n",
        "try:\n",
        "    result = app.invoke(inputs, config=config)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üéØ FINAL OUTPUT: COLD OUTREACH DRAFT\")\n",
        "    print(\"=\"*50)\n",
        "    print(result['final_email'])\n",
        "    print(\"=\"*50)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Execution failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIBCnHiKBylX",
        "outputId": "a1f48966-818f-487a-d2f7-42eabbe52f15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting sequence for Spotify...\n",
            "\n",
            "üîé Researching: Spotify...\n",
            "üß† Analyzing data...\n",
            "‚úçÔ∏è Drafting email...\n",
            "\n",
            "==================================================\n",
            "üéØ FINAL OUTPUT: COLD OUTREACH DRAFT\n",
            "==================================================\n",
            "Subject: Accelerating Spotify's Sustainable Growth\n",
            "\n",
            "Hi [VP's Name],\n",
            "\n",
            "I imagine a key focus for you is driving sustainable revenue growth while building Spotify into an enduring, truly great business for the long term.\n",
            "\n",
            "My firm specializes in helping marketing VPs achieve these exact objectives by uncovering new pathways to accelerate growth and secure lasting market leadership.\n",
            "\n",
            "Would you be open to a brief 15-minute discussion next week to see if there's a fit?\n",
            "\n",
            "Best,\n",
            "[My Name]\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New Code**"
      ],
      "metadata": {
        "id": "Q5XIkSJSXPXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Step 1: Install Dependencies & Configure Gemini\n",
        "\n",
        "!pip install -U google-generativeai langchain-google-genai langgraph langchain_community duckduckgo-search requests > /dev/null\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "import time\n",
        "import json\n",
        "from typing import TypedDict, List, Dict, Any\n",
        "\n",
        "import requests\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.prompts import ChatPromptTemplate\n"
      ],
      "metadata": {
        "id": "v9mpKPZNXdF7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Setup Gemini API Key & Pick Model\n",
        "\n",
        "# 1. Get / set API key (DO NOT hardcode in code you commit)\n",
        "if \"GOOGLE_API_KEY\" not in os.environ or not os.environ[\"GOOGLE_API_KEY\"]:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google Gemini API Key: \")\n",
        "\n",
        "api_key = os.environ[\"GOOGLE_API_KEY\"]\n",
        "\n",
        "def pick_gemini_model(api_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Pick a reasonable Gemini model for chat.\n",
        "    If the API call fails, fall back to a safe default.\n",
        "    \"\"\"\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/models\"\n",
        "    try:\n",
        "        resp = requests.get(base_url, params={\"key\": api_key}, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        models = resp.json().get(\"models\", [])\n",
        "        # Prefer latest flash/pro chat models\n",
        "        preferred_prefixes = [\n",
        "            \"gemini-1.5-flash\",\n",
        "            \"gemini-1.5-pro\",\n",
        "            \"gemini-1.0-pro\",\n",
        "            \"gemini-1.0-pro-vision\",\n",
        "        ]\n",
        "        for prefix in preferred_prefixes:\n",
        "            for m in models:\n",
        "                if m[\"name\"].startswith(prefix):\n",
        "                    print(f\" Using Gemini model: {m['name']}\")\n",
        "                    return m[\"name\"]\n",
        "    except Exception as e:\n",
        "        print(f\" Model discovery failed, using default. Reason: {e}\")\n",
        "\n",
        "    # Safe default (update if needed)\n",
        "    default_model = \"gemini-1.5-flash\"\n",
        "    print(f\" Falling back to default Gemini model: {default_model}\")\n",
        "    return default_model\n",
        "\n",
        "# üîë Choose a valid Gemini chat model explicitly\n",
        "\n",
        "# Common good choices (try these in order if you get 404):\n",
        "# - \"gemini-1.5-flash-002\"\n",
        "# - \"gemini-1.5-flash-001\"\n",
        "# - \"gemini-1.5-flash-8b\"\n",
        "\n",
        "ACTIVE_MODEL = \"gemini-2.5-flash\"\n",
        "print(f\" Using Gemini model: {ACTIVE_MODEL}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzMES4RPd8FA",
        "outputId": "50205f18-28ca-439b-eaaa-f8f7eee4d10c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using Gemini model: gemini-2.5-flash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Define Agent State, LLM, and Search Tool\n",
        "\n",
        "class AgentState(TypedDict, total=False):\n",
        "    company_name: str\n",
        "    target_role: str\n",
        "    research_data: str          # raw search output / summary\n",
        "    strategy_points: str        # bullet points of business problems\n",
        "    final_email: str            # final outreach email\n",
        "    evaluation: str             # evaluation JSON or text\n",
        "    metrics: Dict[str, float]   # simple timing metrics\n",
        "\n",
        "\n",
        "# Shared Gemini LLM for all agents\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=ACTIVE_MODEL,\n",
        "    temperature=0.4,\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],  # optional if env var already set\n",
        ")\n",
        "\n",
        "# Web search tool\n",
        "search_tool = DuckDuckGoSearchRun()\n"
      ],
      "metadata": {
        "id": "SUXUPJZ1eBTt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Research Agent\n",
        "\n",
        "research_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a B2B research assistant. Given a company name and raw web search \"\n",
        "        \"results, produce a concise summary of recent strategic business news or \"\n",
        "        \"initiatives (last 1-2 years). Focus on product launches, market expansion, \"\n",
        "        \"monetization shifts, and leadership statements. Avoid generic company descriptions.\"\n",
        "    ),\n",
        "    (\n",
        "        \"user\",\n",
        "        \"Company: {company_name}\\n\\nRaw search results:\\n{search_results}\\n\\n\"\n",
        "        \"Summarize the most relevant business and strategic updates in under 250 words.\"\n",
        "    ),\n",
        "])\n",
        "research_chain = research_prompt | llm\n",
        "\n",
        "def research_agent(state: AgentState) -> AgentState:\n",
        "    t0 = time.time()\n",
        "    company = state[\"company_name\"]\n",
        "    print(f\" Research Agent: Searching news for {company}...\")\n",
        "\n",
        "    # Use DuckDuckGo search as tool\n",
        "    query = f\"latest strategic business news {company} 2024 2025\"\n",
        "    raw_results = search_tool.invoke(query)\n",
        "\n",
        "    # Make it human-readable for the LLM\n",
        "    search_text = raw_results if isinstance(raw_results, str) else json.dumps(raw_results, ensure_ascii=False)[:4000]\n",
        "\n",
        "    summary = research_chain.invoke({\n",
        "        \"company_name\": company,\n",
        "        \"search_results\": search_text,\n",
        "    }).content\n",
        "\n",
        "    duration = time.time() - t0\n",
        "    metrics = dict(state.get(\"metrics\", {}))\n",
        "    metrics[\"research_duration_sec\"] = duration\n",
        "\n",
        "    print(f\" Research Agent done in {duration:.2f}s\\n\")\n",
        "\n",
        "    return {\n",
        "        \"research_data\": summary,\n",
        "        \"metrics\": metrics,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "TpmqFwh9eFyh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Analysis Agent\n",
        "\n",
        "analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a strategy analyst. Given a summary of recent news about a company, \"\n",
        "        \"identify exactly 2 key business problems or strategic priorities they seem to be focused on.\\n\\n\"\n",
        "        \"Return ONLY bullet points, no extra commentary.\"\n",
        "    ),\n",
        "    (\n",
        "        \"user\",\n",
        "        \"Company: {company_name}\\n\\nNews summary:\\n{research_data}\\n\\n\"\n",
        "        \"Identify 2 key business problems or strategic priorities.\"\n",
        "    ),\n",
        "])\n",
        "analysis_chain = analysis_prompt | llm\n",
        "\n",
        "def analysis_agent(state: AgentState) -> AgentState:\n",
        "    t0 = time.time()\n",
        "    print(\" Analysis Agent: Extracting key business problems...\")\n",
        "\n",
        "    company = state[\"company_name\"]\n",
        "    research_data = state[\"research_data\"]\n",
        "\n",
        "    strategy_points = analysis_chain.invoke({\n",
        "        \"company_name\": company,\n",
        "        \"research_data\": research_data,\n",
        "    }).content\n",
        "\n",
        "    duration = time.time() - t0\n",
        "    metrics = dict(state.get(\"metrics\", {}))\n",
        "    metrics[\"analysis_duration_sec\"] = duration\n",
        "\n",
        "    print(f\" Analysis Agent done in {duration:.2f}s\\n\")\n",
        "\n",
        "    return {\n",
        "        \"strategy_points\": strategy_points,\n",
        "        \"metrics\": metrics,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "rv4JjKZweM2n"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Copywriter Agent\n",
        "\n",
        "copywriter_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a world-class B2B SDR copywriter. Your goal is to write short, \"\n",
        "        \"highly personalized cold emails that reference the prospect's company priorities explicitly.\\n\\n\"\n",
        "        \"Constraints:\\n\"\n",
        "        \"- Under 100 words.\\n\"\n",
        "        \"- Conversational, not overly formal.\\n\"\n",
        "        \"- Reference the business problems explicitly.\\n\"\n",
        "        \"- Clear call to action for a quick call or reply.\"\n",
        "    ),\n",
        "    (\n",
        "        \"user\",\n",
        "        \"Write a cold email to the {target_role} at {company_name}.\\n\\n\"\n",
        "        \"Use these business problems / priorities for personalization:\\n{strategy_points}\\n\\n\"\n",
        "        \"Return only the email body, no subject line, no extra commentary.\"\n",
        "    ),\n",
        "])\n",
        "copywriter_chain = copywriter_prompt | llm\n",
        "\n",
        "def copywriter_agent(state: AgentState) -> AgentState:\n",
        "    t0 = time.time()\n",
        "    company = state[\"company_name\"]\n",
        "    role = state[\"target_role\"]\n",
        "    print(f\" Copywriter Agent: Drafting email for {role} at {company}...\")\n",
        "\n",
        "    email_body = copywriter_chain.invoke({\n",
        "        \"company_name\": company,\n",
        "        \"target_role\": role,\n",
        "        \"strategy_points\": state[\"strategy_points\"],\n",
        "    }).content\n",
        "\n",
        "    duration = time.time() - t0\n",
        "    metrics = dict(state.get(\"metrics\", {}))\n",
        "    metrics[\"copywriter_duration_sec\"] = duration\n",
        "\n",
        "    print(f\" Copywriter Agent done in {duration:.2f}s\\n\")\n",
        "\n",
        "    return {\n",
        "        \"final_email\": email_body,\n",
        "        \"metrics\": metrics,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "xBcqeOKneR62"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Evaluator Agent (Scoring & Feedback) ‚Äì FIXED ESCAPING\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "evaluator_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        # NOTE: all literal { } in the JSON example are escaped as {{ }}\n",
        "        \"You are an expert sales coach and copy evaluator.\\n\"\n",
        "        \"Given the company, target role, business problems, and a cold email, \"\n",
        "        \"evaluate the email and return STRICT JSON with this structure:\\n\\n\"\n",
        "        \"{{\\n\"\n",
        "        '  \"score_relevance\": <integer 0-10>,\\n'\n",
        "        '  \"score_personalization\": <integer 0-10>,\\n'\n",
        "        '  \"score_clarity\": <integer 0-10>,\\n'\n",
        "        '  \"overall_score\": <integer 0-10>,\\n'\n",
        "        '  \"strengths\": [\"...\"],\\n'\n",
        "        '  \"weaknesses\": [\"...\"],\\n'\n",
        "        '  \"suggestions\": [\"...\"]\\n'\n",
        "        \"}}\\n\\n\"\n",
        "        \"Return only valid JSON, no extra text.\"\n",
        "    ),\n",
        "    (\n",
        "        \"user\",\n",
        "        \"Company: {company_name}\\n\"\n",
        "        \"Target role: {target_role}\\n\\n\"\n",
        "        \"Business problems / priorities:\\n{strategy_points}\\n\\n\"\n",
        "        \"Cold email:\\n{final_email}\"\n",
        "    ),\n",
        "])\n",
        "\n",
        "evaluator_chain = evaluator_prompt | llm\n",
        "\n",
        "def evaluator_agent(state: AgentState) -> AgentState:\n",
        "    t0 = time.time()\n",
        "    print(\" Evaluator Agent: Scoring email quality...\")\n",
        "\n",
        "    raw_eval = evaluator_chain.invoke({\n",
        "        \"company_name\": state[\"company_name\"],\n",
        "        \"target_role\": state[\"target_role\"],\n",
        "        \"strategy_points\": state[\"strategy_points\"],\n",
        "        \"final_email\": state[\"final_email\"],\n",
        "    }).content\n",
        "\n",
        "    duration = time.time() - t0\n",
        "    metrics = dict(state.get(\"metrics\", {}))\n",
        "    metrics[\"evaluator_duration_sec\"] = duration\n",
        "\n",
        "    # Try to parse JSON; if it fails, keep raw text so notebook doesn't crash\n",
        "    try:\n",
        "        parsed = json.loads(raw_eval)\n",
        "        pretty = json.dumps(parsed, indent=2)\n",
        "    except Exception:\n",
        "        pretty = raw_eval\n",
        "\n",
        "    print(f\" Evaluator Agent done in {duration:.2f}s\\n\")\n",
        "\n",
        "    return {\n",
        "        \"evaluation\": pretty,\n",
        "        \"metrics\": metrics,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "scAsnZ4UeWQL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Build Multi-Agent Graph with MemorySaver\n",
        "\n",
        "# Build the StateGraph\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "graph.add_node(\"researcher\", research_agent)\n",
        "graph.add_node(\"analyst\", analysis_agent)\n",
        "graph.add_node(\"copywriter\", copywriter_agent)\n",
        "graph.add_node(\"evaluator\", evaluator_agent)\n",
        "\n",
        "graph.set_entry_point(\"researcher\")\n",
        "graph.add_edge(\"researcher\", \"analyst\")\n",
        "graph.add_edge(\"analyst\", \"copywriter\")\n",
        "graph.add_edge(\"copywriter\", \"evaluator\")\n",
        "graph.add_edge(\"evaluator\", END)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = graph.compile(checkpointer=memory)\n",
        "\n",
        "\n",
        "print(\" LangGraph app compiled with MemorySaver.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu8gmNCCeZ4I",
        "outputId": "603654a1-ce9b-4fd3-ac97-30456c8f3f3d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LangGraph app compiled with MemorySaver.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Run DealCloser Once (New Session)\n",
        "\n",
        "target_company = \"Spotify\"        # @param {type:\"string\"}\n",
        "target_person_role = \"VP of Marketing\"   # @param {type:\"string\"}\n",
        "\n",
        "thread_id = \"dealcloser_demo_v1\"  # you can change per run / per account\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "inputs: AgentState = {\n",
        "    \"company_name\": target_company,\n",
        "    \"target_role\": target_person_role,\n",
        "    \"research_data\": \"\",\n",
        "    \"strategy_points\": \"\",\n",
        "    \"final_email\": \"\",\n",
        "    \"evaluation\": \"\",\n",
        "    \"metrics\": {},\n",
        "}\n",
        "\n",
        "print(f\" Starting sequence for {target_company} ({target_person_role})...\\n\")\n",
        "\n",
        "t0 = time.time()\n",
        "result: AgentState = app.invoke(inputs, config=config)\n",
        "total_duration = time.time() - t0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" FINAL OUTPUT: COLD OUTREACH DRAFT\")\n",
        "print(\"=\" * 60)\n",
        "print(result[\"final_email\"])\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "print(\" EVALUATION (JSON-like):\")\n",
        "print(result[\"evaluation\"])\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "print(\" METRICS:\")\n",
        "metrics = dict(result.get(\"metrics\", {}))\n",
        "metrics[\"total_pipeline_duration_sec\"] = total_duration\n",
        "print(json.dumps(metrics, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FNblCcgedU-",
        "outputId": "ab6ec280-a5f1-4c5d-d4e9-91c63a85716f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting sequence for Spotify (VP of Marketing)...\n",
            "\n",
            " Research Agent: Searching news for Spotify...\n",
            " Research Agent done in 6.96s\n",
            "\n",
            " Analysis Agent: Extracting key business problems...\n",
            " Analysis Agent done in 1.78s\n",
            "\n",
            " Copywriter Agent: Drafting email for VP of Marketing at Spotify...\n",
            " Copywriter Agent done in 6.88s\n",
            "\n",
            " Evaluator Agent: Scoring email quality...\n",
            " Evaluator Agent done in 13.66s\n",
            "\n",
            "\n",
            "============================================================\n",
            " FINAL OUTPUT: COLD OUTREACH DRAFT\n",
            "============================================================\n",
            "Hi [VP Marketing Name],\n",
            "\n",
            "I imagine **accelerating subscriber growth and monetization** is a constant focus for you at Spotify. I also understand the critical need for **ensuring long-term business sustainability and operational efficiency**.\n",
            "\n",
            "My team helps marketing leaders tackle these exact challenges, and I believe we could offer some valuable insights for Spotify.\n",
            "\n",
            "Would you be open to a quick 15-minute chat next week to explore this further?\n",
            "============================================================\n",
            "\n",
            " EVALUATION (JSON-like):\n",
            "```json\n",
            "{\n",
            "  \"score_relevance\": 9,\n",
            "  \"score_personalization\": 6,\n",
            "  \"score_clarity\": 7,\n",
            "  \"overall_score\": 6,\n",
            "  \"strengths\": [\n",
            "    \"Directly addresses the target's known business problems/priorities, using their exact language.\",\n",
            "    \"Highly concise and respects the recipient's time.\",\n",
            "    \"Clear and unambiguous call to action.\",\n",
            "    \"Good use of company and role-specific language.\"\n",
            "  ],\n",
            "  \"weaknesses\": [\n",
            "    \"The value proposition (\\\"valuable insights\\\") is generic and lacks specificity.\",\n",
            "    \"Doesn't provide any proof points, examples, or unique insights to build credibility or differentiate the sender.\",\n",
            "    \"Could feel templated, as the problem statements are directly copied without further context or unique observation.\",\n",
            "    \"No subject line provided (a critical component for open rates).\",\n",
            "    \"Doesn't offer a strong differentiator for the sender's team.\"\n",
            "  ],\n",
            "  \"suggestions\": [\n",
            "    \"Craft a compelling and relevant subject line (e.g., \\\"Idea for Spotify: Accelerating Subscriber Growth\\\" or \\\"[Your Company] + Spotify: Operational Efficiency\\\").\",\n",
            "    \"Instead of 'valuable insights,' hint at a specific, unique approach, a relevant case study, or a specific outcome (e.g., 'We recently helped [similar company] increase monetization by X% through Y strategy').\",\n",
            "    \"Offer a specific, actionable piece of information or a unique perspective related to one of Spotify's challenges within the email itself to pique interest.\",\n",
            "    \"Make the benefit of the 15-minute chat more explicit (e.g., 'to share 3 strategies we've seen drive X for similar platforms' or 'to discuss a specific framework for improving operational efficiency').\",\n",
            "    \"Consider a softer call to action, such as offering a relevant resource or a brief diagnostic, before immediately asking for a meeting.\"\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "============================================================\n",
            "‚è± METRICS:\n",
            "{\n",
            "  \"research_duration_sec\": 6.960421562194824,\n",
            "  \"analysis_duration_sec\": 1.7847990989685059,\n",
            "  \"copywriter_duration_sec\": 6.882131099700928,\n",
            "  \"evaluator_duration_sec\": 13.6610689163208,\n",
            "  \"total_pipeline_duration_sec\": 29.310324907302856\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}